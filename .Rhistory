# QDA on the best model, which has 11 predictors
library(MASS)
qda.fit = qda(output ~ sex+cp+trtbps+chol+restecg+thalachh+exng+oldpeak+slp+caa+thall,
data = heart_dat, subset = train)
qda.fit
library(bestglm)
anna_model <- glm(output~., data = heart_dat, family = binomial)
anna_backwards <- step(anna_model)
summary(anna_backwards)
anna_forwards <- step(anna_model, direction = "forward")
summary(anna_forwards)
anna_best <- step(anna_model, direction = "both")
summary(anna_best)
dt = sort(sample(nrow(heart_dat), nrow(heart_dat)*.9))
train<-heart_dat[dt,]
test<-heart_dat[-dt,]
anna.best <- regsubsets(output~., data = train, nvmax = 13 )
test.matrix <- model.matrix(output~., data = test)
val.errors=rep(NA,13)
for(i in c(1:13)){
coefi=coef(anna.best,id=i)
pred=test.matrix[,names(coefi)]%*%coefi
val.errors[i]=mean((test$output-pred)^2)
}
which.min(val.errors)
best_subset_summ <- summary(best_subset_fit)
which.min(best_subset_summ$bic)
which.min(best_subset_summ$cp)
forward_step_summ <- summary(forward_step_fit)
which.min(forward_step_summ$bic)
which.min(forward_step_summ$cp)
back_step_summ <- summary(backward_step_fit)
which.min(back_step_summ$bic)
which.min(back_step_summ$cp)
stepAIC(best_subset_fit)
which.min(best_subset_summ$aic)
which.min(best_subset_summ$bic)
which.min(best_subset_summ$cp)
heart_dat <- read.csv("heart.csv")
head(heart_dat)
# lasso
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
y.test=y[test]
mean((lasso.pred-y.test)^2)
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
install.packages("caret")
library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
train_samples <- heart_dat %>% createDataPartition(p = 0.9, list = FALSE)
library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
train_samples <- heart_dat %>%
createDataPartition(p = 0.9, list = FALSE)
library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
full.model <- glm(output ~., data = heart_dat[train], family = binomial) %>%
stepAIC(trace = FALSE)
train
heart_dat[train]
train
heart_dat[train]
heart_dat[train,]
library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
full.model <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
full.model
# fitting a model with best subset selection
library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
best_subset_logregmodel
# fitting a model with forward stepwise selection
forward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "forward", trace = FALSE)
forward_logregmodel
```{r}
# fitting a model with backward stepwise selection
backward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "backward", trace = FALSE)
backward_logregmodel
heart_dat <- read.csv("heart.csv")
head(heart_dat)
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
y.test=y[test]
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
y=heart_dat$output
y.test=y[test]
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
# fitting a model with best subset selection
library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
best_subset_logregmodel
# fitting a model with best subset selection
library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
best_subset_logregmodel
# fitting a model with forward stepwise selection
forward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "forward", trace = FALSE)
forward_logregmodel
# fitting a model with backward stepwise selection
backward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "backward", trace = FALSE)
backward_logregmodel
# finding best model from variable selection methods using 10-fold CV
k = 10
set.seed(1)
folds = sample(1:k, nrow(heart_dat), replace = TRUE)
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
which.min(mean.cv.errors)
# finding best model from variable selection methods using 5-fold CV
k2 = 5
set.seed(1)
folds = sample(1:k2, nrow(heart_dat), replace = TRUE)
cv.errors = matrix(NA, k2, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k2){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors2[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
# finding best model from variable selection methods using 5-fold CV
k2 = 5
set.seed(1)
folds = sample(1:k2, nrow(heart_dat), replace = TRUE)
cv.errors2 = matrix(NA, k2, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k2){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors2[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
mean.cv.errors2 = apply(cv.errors2, 2, mean)
which.min(mean.cv.errors2)
# finding best model from variable selection methods using 10-fold CV
k = 10
set.seed(1)
folds = sample(1:k, nrow(heart_dat), replace = TRUE)
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
# finding best model from variable selection methods using 10-fold CV
k = 10
set.seed(1)
folds = sample(1:k, nrow(heart_dat), replace = TRUE)
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
#mean.cv.errors = apply(cv.errors, 2, mean)
cv.errors
# finding best model from variable selection methods using 10-fold CV
k = 10
set.seed(1)
folds = sample(1:k, nrow(heart_dat), replace = TRUE)
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
# finding best model from variable selection methods using 10-fold CV
k = 10
set.seed(1)
folds = sample(1:k, nrow(heart_dat), replace = TRUE)
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
#mean.cv.errors = apply(cv.errors, 2, mean)
cv.errors
# finding best model from variable selection methods using 5-fold CV
k2 = 5
set.seed(1)
folds = sample(1:k2, nrow(heart_dat), replace = TRUE)
cv.errors2 = matrix(NA, k2, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k2){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors2[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
mean.cv.errors2 = apply(cv.errors2, 2, mean)
which.min(mean.cv.errors2)
# finding best model from variable selection methods using 5-fold CV
k2 = 5
set.seed(1)
folds = sample(1:k2, nrow(heart_dat), replace = TRUE)
cv.errors2 = matrix(NA, k2, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k2){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors2[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
#mean.cv.errors2 = apply(cv.errors2, 2, mean)
#which.min(mean.cv.errors2)
cv.errors2
# finding best model from variable selection methods using 10-fold CV
k = 10
set.seed(1)
folds = sample(1:k, nrow(heart_dat), replace = TRUE)
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))
for(j in 1:k){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,],id=i)
cv.errors[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
}
}
mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
# Performing validation set approach
#training = sample(x = 1:nrow(heart_dat), size = nrow(heart_dat)/2)
training = sample(c(TRUE, FALSE), nrow(heart_dat), rep = TRUE)
testing = (!train)
training_dat <- heart_dat[training,]
testing_dat <- heart_dat[-training,]
anna.best2 <- regsubsets(output~., data = training_dat, nvmax = 13 )
cv.error.10= rep(0,10)
for (i in 1:10) {
cv.error.10[i]=cv.glm(Auto,best_subset_logregmodel,K=10)$delta[1]
}
library(boot)
cv.error.10= rep(0,10)
for (i in 1:10) {
cv.error.10[i]=cv.glm(Auto,best_subset_logregmodel,K=10)$delta[1]
}
library(boot)
cv.error.10= rep(0,10)
for (i in 1:10) {
cv.error.10[i]=cv.glm(heart_dat,best_subset_logregmodel,K=10)$delta[1]
}
cv.error.10
cv.error.10= rep(0,10)
for (i in 1:10) {
cv.error.10[i]=cv.glm(heart_dat,best_subset_logregmodel,K=10)$delta[1]
}
cv.error.10
anna_best_final <- glm(output~ sex + cp + thalachh + oldpeak + caa + thall, data = train, family = binomial)
library(bestglm)
heart_dat2 <- heart_dat
names(heart_dat2)[14] <- "y"
best <- bestglm(heart_dat2, family = binomial, IC = "CV")
anna_best_final <- glm(output~ sex + cp + thalachh + oldpeak + caa + thall, data = train, family = binomial)
knitr::opts_chunk$set(echo = TRUE)
heart <- read.csv("heart.csv")
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
heart <- read.csv("heart.csv")
library(ggplot2)
library(tidyverse)
hist(heart$age)
# reading in the data set and viewing the first few rows
heart_attack_dat <- read.csv("heart.csv")
heart_attack_dat_2 <- heart_attack_dat[-86,]
heart_attack_dat_2
# Comparing relationship between Heart Attack Incidence (0/1) and Cholesterol
# Level
library(ggplot2)
ggplot(data = heart_attack_dat_2, aes(x = chol, y = output)) +
geom_point() +
ggtitle("Did Heart Attack Happen vs. Cholesterol Level") +
xlab("Cholesterol Level") +
ylab("Did Heart Attack Happen")
# getting summary statistics for each of the numeric type predictor variables to
# be used from the data set
library(stargazer)
stargazer(heart_attack_dat_2[c("age","trtbps","chol","thalachh","oldpeak")], type="text",summary.stat=c("n","mean","sd", "min", "max"))
ggplot(data = heart, aes(x = age, y = chol)) + geom_point() + stat_smooth(method = lm) +
facet_wrap(.~sex) + labs(main = "chol v age for men and women")
heart %>% group_by(sex) %>% summarise( total = n())
heart %>% group_by(cp, output) %>% summarise(total = n())
round(cov(heart), 2)
round(cor(heart), 2)
ggplot(data = heart, aes(x = age, y = cp)) + geom_point()
ggplot(data = heart, aes(x = age, y = caa)) + geom_point()
heart %>% group_by(fbs, output) %>% summarise( total = n())
heart %>% group_by(exng, restecg) %>% summarise(total = n())
heart_dat <- read.csv("heart.csv")
head(heart_dat)
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
y=heart_dat$output
y.test=y[test]
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE
min <- which.min(cv.out$lambda)
coef(lasso.mod)[,min]
cv.out
min <- which.min(cv.out$lambda.min)
coef(lasso.mod)[,min]
min <- which.min(cv.out$lambda)
coef(lasso.mod)[,min]
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
coef(ridge.mod)[,1]
ridge.mod
# fitting a model with best subset selection
#library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
best_subset_logregmodel
# fitting a model with best subset selection
#library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
best_subset_logregmodel
# fitting a model with best subset selection
#library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
best_subset_logregmodel
# fitting a model with forward stepwise selection
forward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "forward", trace = FALSE)
forward_logregmodel
# fitting a model with backward stepwise selection
backward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "backward", trace = FALSE)
backward_logregmodel
summary(forward_logregmodel)
forward_logreg_model
forward_logregmodel
summary(forward_logregmodel)
best_subset_logregmodel
summary(best_subset_logregmodel)
# LDA on the best model (with the lowest cross-validation error), which was the full model
library(MASS)
lda.fit = lda(output ~ .,
data = heart_dat, subset = train)
lda.fit
# QDA on the best model, which has 11 predictors
library(MASS)
qda.fit = qda(output ~ sex+cp+trtbps+chol+restecg+thalachh+exng+oldpeak+slp+caa+thall,
data = heart_dat, subset = train)
qda.fit
dt = sort(sample(nrow(heart_dat), nrow(heart_dat)*.9))
train<-heart_dat[dt,]
test<-heart_dat[-dt,]
lda_best <- lda(output~ sex + cp + trtbps + fbs + exng + oldpeak + slp + caa + thall,
data = train)
lda_best_pred <- predict(lda_best, test)
table(lda_best_pred$class, test$output)
qda_best <- qda(output~ sex + cp + trtbps + fbs + exng + oldpeak + slp + caa + thall,
data = train)
qda_best_pred <- predict(qda_best, test)
table(qda_best_pred$class, test$output)
