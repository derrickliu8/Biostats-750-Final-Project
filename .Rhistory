hist(heart$age)
# reading in the data set and viewing the first few rows
heart_attack_dat <- read.csv("heart.csv")
heart_attack_dat_2 <- heart_attack_dat[-86,]
heart_attack_dat_2
# Comparing relationship between Heart Attack Incidence (0/1) and Cholesterol
# Level
library(ggplot2)
ggplot(data = heart_attack_dat_2, aes(x = chol, y = output)) +
geom_point() +
ggtitle("Did Heart Attack Happen vs. Cholesterol Level") +
xlab("Cholesterol Level") +
ylab("Did Heart Attack Happen")
# getting summary statistics for each of the numeric type predictor variables to
# be used from the data set
library(stargazer)
stargazer(heart_attack_dat_2[c("age","trtbps","chol","thalachh","oldpeak")], type="text",summary.stat=c("n","mean","sd", "min", "max"))
ggplot(data = heart, aes(x = age, y = chol)) + geom_point() + stat_smooth(method = lm) +
facet_wrap(.~sex) + labs(main = "chol v age for men and women")
heart %>% group_by(sex) %>% summarise( total = n())
heart %>% group_by(cp, output) %>% summarise(total = n())
round(cov(heart), 2)
round(cor(heart), 2)
ggplot(data = heart, aes(x = age, y = cp)) + geom_point()
ggplot(data = heart, aes(x = age, y = caa)) + geom_point()
heart %>% group_by(fbs, output) %>% summarise( total = n())
heart %>% group_by(exng, restecg) %>% summarise(total = n())
heart_dat <- read.csv("heart.csv")
head(heart_dat)
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
y=heart_dat$output
y.test=y[test]
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE
min <- which.min(cv.out$lambda)
coef(lasso.mod)[,min]
cv.out
min <- which.min(cv.out$lambda.min)
coef(lasso.mod)[,min]
min <- which.min(cv.out$lambda)
coef(lasso.mod)[,min]
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
coef(ridge.mod)[,1]
ridge.mod
# fitting a model with best subset selection
#library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
best_subset_logregmodel
# fitting a model with best subset selection
#library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
best_subset_logregmodel
# fitting a model with best subset selection
#library(caret)
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
#train_samples <- heart_dat %>%
#createDataPartition(p = 0.9, list = FALSE)
#train.data <- heart_dat[train_samples]
#test.data <- heart_dat[-train_samples]
#full.model <- glm(output ~ ., data = train.data, family = binomial) %>%
#stepAIC(trace = FALSE)
best_subset_logregmodel
# fitting a model with forward stepwise selection
forward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "forward", trace = FALSE)
forward_logregmodel
# fitting a model with backward stepwise selection
backward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "backward", trace = FALSE)
backward_logregmodel
summary(forward_logregmodel)
forward_logreg_model
forward_logregmodel
summary(forward_logregmodel)
best_subset_logregmodel
summary(best_subset_logregmodel)
# LDA on the best model (with the lowest cross-validation error), which was the full model
library(MASS)
lda.fit = lda(output ~ .,
data = heart_dat, subset = train)
lda.fit
# QDA on the best model, which has 11 predictors
library(MASS)
qda.fit = qda(output ~ sex+cp+trtbps+chol+restecg+thalachh+exng+oldpeak+slp+caa+thall,
data = heart_dat, subset = train)
qda.fit
dt = sort(sample(nrow(heart_dat), nrow(heart_dat)*.9))
train<-heart_dat[dt,]
test<-heart_dat[-dt,]
lda_best <- lda(output~ sex + cp + trtbps + fbs + exng + oldpeak + slp + caa + thall,
data = train)
lda_best_pred <- predict(lda_best, test)
table(lda_best_pred$class, test$output)
qda_best <- qda(output~ sex + cp + trtbps + fbs + exng + oldpeak + slp + caa + thall,
data = train)
qda_best_pred <- predict(qda_best, test)
table(qda_best_pred$class, test$output)
heart_dat <- read.csv("heart.csv")
head(heart_dat)
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
y=heart_dat$output
y.test=y[test]
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE
min <- which.min(cv.out$lambda)
coef(lasso.mod)[,min]
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
coef(ridge.mod)[,1]
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
y=heart_dat$output
y.test=y[test]
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=as.factor(heart_dat$output)
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
y=heart_dat$output
y.test=y[test]
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
lasso.mod=glmnet(x[train,],as.factor(y[train]),alpha=1,lambda=grid)
heart_dat$output <- as.factor(heart_dat$output)
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
y=heart_dat$output
y.test=y[test]
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
heart_dat <- read.csv("heart.csv")
head(heart_dat)
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
y=heart_dat$output
y.test=y[test]
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid, family = binomial)
plot(lasso.mod)
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE
min <- which.min(cv.out$lambda)
coef(lasso.mod)[,min]
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1, family = binomial)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE
min <- which.min(cv.out$lambda)
coef(lasso.mod)[,min]
min <- which.min(cv.out$lambda.min)
coef(lasso.mod)[,min]
min <- which.min(cv.out$lambda.min)
coef(lasso.mod)[,1]
min <- which.min(cv.out$lambda.min)
coef(lasso.mod)[,2]
min <- which.min(cv.out$lambda.min)
coef(lasso.mod)[,10]
min <- which.min(cv.out$lambda.min)
coef(lasso.mod)[,min]
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12, family = binomial)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
coef(ridge.mod)[,1]
min <- which.min(cv.out$lambda)
coef(lasso.mod)[,min]
heart_dat <- read.csv("heart.csv")
head(heart_dat)
heart_dat$sex <- as.factor(heart_dat$sex)
heart_dat$cp <- as.factor(heart_dat$cp)
heart_dat$fbs <- as.factor(heart_dat$fbs)
heart_dat$restecg <- as.factor(heart_dat$restecg)
heart_dat$exng <- as.factor(heart_dat$exng)
heart_dat$slp <- as.factor(heart_dat$slp)
heart_dat$caa <- as.factor(heart_dat$caa)
heart_dat$thall <- as.factor(heart_dat$thall)
heart_dat$output <- as.factor(heart_dat$output)
heart_dat
head(heart_dat)
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
y.test=y[test]
#lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid, family = binomial)
# reading in the data set
heart_dat <- read.csv("heart.csv")
head(heart_dat)
# changing all the non-numeric variables to factor type variables
heart_dat$sex <- as.factor(heart_dat$sex)
heart_dat$cp <- as.factor(heart_dat$cp)
heart_dat$fbs <- as.factor(heart_dat$fbs)
heart_dat$restecg <- as.factor(heart_dat$restecg)
heart_dat$exng <- as.factor(heart_dat$exng)
heart_dat$slp <- as.factor(heart_dat$slp)
heart_dat$caa <- as.factor(heart_dat$caa)
heart_dat$thall <- as.factor(heart_dat$thall)
#heart_dat$output <- as.factor(heart_dat$output)
head(heart_dat)
# lasso - code adapted from Lab 5
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=as.factor(heart_dat$output)
y.test=y[test]
#lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid, family = binomial)
set.seed(1)
#cv.out=cv.glmnet(x[train,],y[train],alpha=1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1, family = binomial)
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
# reading in the data set
heart_dat <- read.csv("heart.csv")
head(heart_dat)
# changing all the non-numeric variables to factor type variables
heart_dat$sex <- as.factor(heart_dat$sex)
heart_dat$cp <- as.factor(heart_dat$cp)
heart_dat$fbs <- as.factor(heart_dat$fbs)
heart_dat$restecg <- as.factor(heart_dat$restecg)
heart_dat$exng <- as.factor(heart_dat$exng)
heart_dat$slp <- as.factor(heart_dat$slp)
heart_dat$caa <- as.factor(heart_dat$caa)
heart_dat$thall <- as.factor(heart_dat$thall)
#heart_dat$output <- as.factor(heart_dat$output)
head(heart_dat)
# lasso - code adapted from Lab 5
# binomial still not working here
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
y.test=y[test]
#lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid, family = binomial)
# lasso - code adapted from Lab 5
# binomial still not working here
library(glmnet)
train=sample(1:nrow(heart_dat), nrow(heart_dat)/2)
test = (-train)
grid=10^seq(10,-2,length=100)
x=model.matrix(output~.,heart_dat)[,-14]
y=heart_dat$output
y.test=y[test]
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
#lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid, family = binomial)
plot(lasso.mod)
# continuing lasso - code adapted from Lab 5
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
#cv.out=cv.glmnet(x[train,],y[train],alpha=1, family = binomial)
plot(cv.out)
bestlam=cv.out$lambda.min # tuning parameter
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) # MSE
out = glmnet(x,y,alpha=1,lambda=grid) # producing a model with 10 predictors
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:13,]
lasso.coef[lasso.coef!=0]
bestlam
cv.out$lambda
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
#ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12,
#family = binomial)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
out2=glmnet(x,y,alpha=0)  # again, glmnet function is producing a model with not all zero values for the parameter estimates
# glmnet function fits a GLM with lasso or elastic net regularization
ridge.coef = predict(out2,type="coefficients",s=bestlam)[1:13,]
ridge.coef[ridge.coef!=0]
# ridge - code adapted from Lab 5
#ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12,
#family = binomial)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
# ridge - code adapted from Lab 5
#ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12,
#family = binomial)
ridge.pred=predict(ridge.mod,newx=x[test,])
#mean((ridge.pred-y.test)^2)
out2=glmnet(x,y,alpha=0)  # again, glmnet function is producing a model with not all zero values for the parameter estimates
# ridge - code adapted from Lab 5
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
#ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12,
#family = binomial)
ridge.pred=predict(ridge.mod,newx=x[test,])
mean((ridge.pred-y.test)^2)
out2=glmnet(x,y,alpha=0)  # again, glmnet function is producing a model with not all zero values for the parameter estimates
# glmnet function fits a GLM with lasso or elastic net regularization
ridge.coef = predict(out2,type="coefficients",s=bestlam)[1:13,]
ridge.coef[ridge.coef!=0]
# fitting a model with best subset selection
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
best_subset_logregmodel
summary(best_subset_logregmodel)
# fitting a model with best subset selection
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
best_subset_logregmodel
summary(best_subset_logregmodel)
# fitting a model with best subset selection
library(MASS) # for the stepAIC function
library(dplyr) # for the pipeline operator
set.seed(1)
best_subset_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(trace = FALSE)
best_subset_logregmodel
summary(best_subset_logregmodel)
# fitting a model with forward stepwise selection
set.seed(1)
forward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "forward", trace = FALSE)
forward_logregmodel
summary(forward_logregmodel)
# fitting a model with backward stepwise selection
set.seed(1)
backward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "backward", trace = FALSE)
backward_logregmodel
# fitting a model with backward stepwise selection
set.seed(1)
backward_logregmodel <- glm(output ~., data = heart_dat[train,], family = binomial) %>%
stepAIC(direction = "backward", trace = FALSE)
backward_logregmodel
summary(backward_logregmodel)
final_logregmodel <- glm(output ~sex+cp+trtbps+thalachh+exng+caa+thall,
data = heart_dat, family = binomial)
summary(final_logregmodel)
final_logregmodel <- glm(output ~sex+cp+trtbps+thalachh+exng+caa+thall,
data = heart_dat, family = binomial)
summary(final_logregmodel)
# Cross Validation done on the model chosen by best subset selection
set.seed(1)
model_saver <- matrix(ncol = 10)
cv.errors2 = matrix(NA, k, 13)
# Cross Validation done on the model chosen by best subset selection
k = 10
set.seed(1)
model_saver <- matrix(ncol = 10)
cv.errors2 = matrix(NA, k, 13)
intercept <- matrix(NA, k, 11)
for(j in 1:k){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
best_subset_logregmodel <- glm(output ~., data = heart_dat[folds != j,], family = binomial) %>%
stepAIC(trace = F)
model_saver[j] <- length(best_subset_logregmodel$coefficients)  #saves the number of non-zero coefficients
intercept[j,] <- best_subset_logregmodel$coefficients[1:11]     #looks at model coefficients
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,], type = "response") #what does id do? id does nothing
pred = ifelse(pred > 0.5, 1, 0)
cv.errors2[j, i]=mean((heart_dat$output[folds==j] != pred))
}
}
# Cross Validation done on the model chosen by best subset selection
k = 10
folds = sample(1:k, nrow(heart_dat), replace = TRUE)
set.seed(1)
model_saver <- matrix(ncol = 10)
cv.errors2 = matrix(NA, k, 13)
intercept <- matrix(NA, k, 11)
for(j in 1:k){
#best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13)
best_subset_logregmodel <- glm(output ~., data = heart_dat[folds != j,], family = binomial) %>%
stepAIC(trace = F)
model_saver[j] <- length(best_subset_logregmodel$coefficients)  #saves the number of non-zero coefficients
intercept[j,] <- best_subset_logregmodel$coefficients[1:11]     #looks at model coefficients
for(i in 1:13){
pred=predict(best_subset_logregmodel,heart_dat[folds==j,], type = "response") #what does id do? id does nothing
pred = ifelse(pred > 0.5, 1, 0)
cv.errors2[j, i]=mean((heart_dat$output[folds==j] != pred))
}
}
mean.cv.errors.anna = mean(cv.errors2)
#cv.errors2[,1]
intercept
#model_saver
mean.cv.errors.anna
# histogram of the age variable
hist(heart$age, main = "Ages", xlab = "Age (in years)")
# histogram of the age variable
hist(heart_dat$age, main = "Ages", xlab = "Age (in years)")
# getting summary statistics for each of the numeric type predictor variables to
# be used from the data set
library(stargazer)
stargazer(heart_attack_dat[c("age","trtbps","chol","thalachh","oldpeak")], type="text",summary.stat=c("n","mean","sd", "min", "max"))
# getting summary statistics for each of the numeric type predictor variables to
# be used from the data set
library(stargazer)
stargazer(heart_dat[c("age","trtbps","chol","thalachh","oldpeak")], type="text",summary.stat=c("n","mean","sd", "min", "max"))
# histogram of the resting blood pressure
hist(heart_dat$trtbps, main = "Resting Blood Pressure",
xlab = "Resting Blood Pressure (in mm Hg)")
# histogram of the cholesterol levels
hist(heart_dat$chol, main = "Cholesterol Levels",
xlab = "Cholesterol (mg/dl)")
knitr::opts_chunk$set(echo = TRUE)
heart <- read.csv("heart.csv")
library(ggplot2)
library(tidyverse)
heart %>% group_by(sex) %>% summarise( total = n())
heart_dat %>% group_by(sex) %>% summarise( total = n())
round(cor(heart), 2)
