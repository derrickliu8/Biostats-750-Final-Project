---
title: "Variable Selection"
author: "Derrick Liu"
date: "4/24/2021"
output: html_document
---

```{r}
heart_dat <- read.csv("heart.csv")
head(heart_dat)
```

# Best subset selection

```{r}
library(leaps)

best_subset_fit <- regsubsets(output ~ ., data = heart_dat, nvmax = 13)
best_subset_summ <- summary(best_subset_fit)
```

```{r}
which.min(best_subset_summ$bic)
which.min(best_subset_summ$cp)
```

```{r}
# Finding the number of predictors in the models selected by best subset selection
# that has the largest R^2

plot(summary(best_subset_fit)$adjr2, xlab = "Number of Variables", 
     ylab = "Adjusted RSq", type = "l")
which.max(summary(best_subset_fit)$adjr2)
points(10, summary(best_subset_fit)$adjr2[10], col = "red", cex = 2, pch = 20)
```

# Forward stepwise selection

```{r}
forward_step_fit <- regsubsets(output ~ ., data = heart_dat, nvmax = 13, 
                               method = "forward")

forward_step_summ <- summary(forward_step_fit)
```

```{r}
# Finding the number of predictors in the models selected by forward stepwise 
# selection that has the largest R^2

plot(summary(forward_step_fit)$adjr2, xlab = "Number of Variables", 
     ylab = "Adjusted RSq", type = "l")
which.max(summary(forward_step_fit)$adjr2)
points(10, summary(forward_step_fit)$adjr2[10], col = "red", cex = 2, pch = 20)
```

```{r}
which.min(forward_step_summ$bic)
which.min(forward_step_summ$cp)
```


# Backward stepwise selection

```{r}
backward_step_fit <- regsubsets(output ~ ., data = heart_dat, nvmax = 13, 
                               method = "backward")

back_step_summ <- summary(backward_step_fit)
```

```{r}
# Finding the number of predictors in the models selected by backward stepwise 
# selection that has the largest R^2

plot(summary(backward_step_fit)$adjr2, xlab = "Number of Variables", 
     ylab = "Adjusted RSq", type = "l")
which.max(summary(backward_step_fit)$adjr2)
points(10, summary(backward_step_fit)$adjr2[10], col = "red", cex = 2, pch = 20)
```

The adjusted R^2 value is the largest for the models with 10 predictors 
selected by each best subset selection, forward stepwise selection, and backward
stepwise selection.

```{r}
which.min(back_step_summ$bic)
which.min(back_step_summ$cp)
```


# Selecting the Best Model via Cross Validation

```{r}
# 10-fold cross validation - code adapted from Lab 4

predict.regsubsets = function (object ,newdata ,id ,...){ 
  form=as.formula(object$call[[2]]) 
  mat=model.matrix(form,newdata) 
  coefi=coef(object ,id=id) 
  xvars=names(coefi) 
  mat[,xvars]%*%coefi 
}

k = 10
set.seed(1)
folds = sample(1:k, nrow(heart_dat), replace = TRUE)
cv.errors = matrix(NA, k, 13, dimnames = list(NULL, paste(1:13)))

for(j in 1:k){ 
  best.fit=regsubsets(output~.,data=heart_dat[folds!=j,], nvmax =13) 
  
  for(i in 1:13){
    pred=predict(best.fit,heart_dat[folds==j,],id=i)
    cv.errors[j,i]=mean((heart_dat$output[folds==j]-pred)^2)
  } 
}

mean.cv.errors = apply(cv.errors, 2, mean)
which.min(mean.cv.errors)
```

Cross validation selects an 11-variable model.

```{r}
plot(mean.cv.errors, type = "b")
```

```{r}
# Finding the best model with 11 predictors using best subset selection

reg.best=regsubsets(output~., data = heart_dat, nvmax=13)
                    
coef(reg.best,11)
```

```{r}
# Finding the best model with 11 predictors using forward stepwise selection

reg.best_fwd=regsubsets(output~., data = heart_dat, nvmax=13, method = "forward")
                    
coef(reg.best_fwd,11)
```

```{r}
# Finding the best model with 11 predictors using backward stepwise selection

reg.best_backwd=regsubsets(output~., data = heart_dat, nvmax=13, method = "backward")
                    
coef(reg.best_backwd,11)
```

Getting same results from best subset selection, forward stepwise selection, 
and backward stepwise selection --> not really sure why --> don't get same 
results 

```{r}
# have to pick an appropriate training data set criteria

train = (heart_dat$trtbps > 131)
```


# logistic regression

```{r}
# model with all 11 predictors selected by cross validation

heart_logreg <- glm(output~sex+cp+trtbps+chol+restecg+thalachh+exng+oldpeak+slp+caa+thall,
                    family = "binomial", data = heart_dat)
summary(heart_logreg)
```

```{r}
# model with just the statistically significant predictors from above

heart_logreg_red <- glm(output~sex+cp+trtbps+thalachh+exng+oldpeak+caa+thall,
                    family = "binomial", data = heart_dat)
summary(heart_logreg_red)
```

All variables that were statistically significant in the model chosen from CV
are still statistically significant. 

# LDA

```{r}
# LDA on the best model, which has 11 predictors

library(MASS)

lda.fit = lda(output ~ sex+cp+trtbps+chol+restecg+thalachh+exng+oldpeak+slp+caa+thall, 
              data = heart_dat, subset = train)

lda.fit
```

The coefficients of linear discriminants shows the linear combination of 
predictor variables that are used to form the LDA decision rule. 

# QDA
```{r}
# QDA on the best model, which has 11 predictors

library(MASS)

qda.fit = qda(output ~ sex+cp+trtbps+chol+restecg+thalachh+exng+oldpeak+slp+caa+thall, 
              data = heart_dat, subset = train)

qda.fit
```





Anna work, backwards model
```{r}

anna_model <- glm(output~., data = heart_dat, family = binomial)

anna_backwards <- step(anna_model)

summary(anna_backwards)

```


forwards model 
```{r}
anna_forwards <- step(anna_model, direction = "forward")

summary(anna_forwards)
```

```{r}
# hybrid of backward and forward
# best subset selection may not be best --> have to consider 2^13 different models
# elastic net and lasso
# variable selection methods --> see how they work on the models that are chosen, not really trying to pick the "best model"

anna_best <- step(anna_model, direction = "both")
summary(anna_best)
```
My best subset model is the same as my backwards selected model.


```{r}
dt = sort(sample(nrow(heart_dat), nrow(heart_dat)*.9))
train<-heart_dat[dt,]
test<-heart_dat[-dt,]

anna.best <- regsubsets(output~., data = train, nvmax = 13 )
test.matrix <- model.matrix(output~., data = test)

val.errors=rep(NA,13)
for(i in c(1:13)){
coefi=coef(anna.best,id=i)
pred=test.matrix[,names(coefi)]%*%coefi
val.errors[i]=mean((test$output-pred)^2)
}
which.min(val.errors)
```
the best number of variables changes everytime.


Bestglm, used for logistic regression, regsubset supposedly only does linear models
```{r}
library(bestglm)
heart_dat2 <- heart_dat
names(heart_dat2)[14] <- "y"

best <- bestglm(heart_dat2, family = binomial, IC = "CV")
summary(best$BestModel)
```
Using bestglm model to see how we do with prediction
```{r}
anna_best_final <- glm(output~ sex + cp + thalachh + oldpeak + caa + thall, data = train, family = binomial)
anna_probs <- predict(anna_best_final, test, type = "response")
anna_pred <- rep(0, nrow(test))
anna_pred[anna_probs > 0.5] <- 1

table(anna_pred, test$output)
```
This gives a 71% accuracy


good old glm to stop making my head spin
```{r}
anna_backwards_final <-glm(formula = output ~ sex + cp + trtbps + restecg + thalachh + 
    exng + oldpeak + slp + caa + thall, family = binomial, data = train)
anna_probs <- predict(anna_backwards_final, test, type = "response")
anna_pred <- rep(0, nrow(test))
anna_pred[anna_probs > 0.5] <- 1

table(anna_pred, test$output)
```
This gives 83.8% accuracy

Using LDA for the best model
```{r}
lda_best <- lda(output~ sex + cp + thalachh + oldpeak + caa + thall, data = train)
lda_best_pred <- predict(lda_best, test)

table(lda_best_pred$class, test$output)
```
Does just slightly better than logistic regression


Using LDA for backwards model
```{r}
lda_backwards <- lda(formula = output ~ sex + cp + trtbps + restecg + thalachh + 
    exng + oldpeak + slp + caa + thall, data = train)
lda_backwards_preds <- predict(lda_backwards, test)

table(lda_backwards_preds$class, test$output)
```
does slightly worse than log reg

```{r}
# Performing validation set approach

#training = sample(x = 1:nrow(heart_dat), size = nrow(heart_dat)/2)

training = sample(c(TRUE, FALSE), nrow(heart_dat), rep = TRUE)
testing = (!train)

training_dat <- heart_dat[training,]
testing_dat <- heart_dat[-training,]

anna.best2 <- regsubsets(output~., data = training_dat, nvmax = 13 )
test.matrix2 <- model.matrix(output~., data = testing_dat)

val.errors2=rep(NA,13)
for(i in c(1:13)){
coefi2=coef(anna.best2,id=i)
pred2=test.matrix2[,names(coefi2)]%*%coefi2
val.errors2[i]=mean((testing_dat$output-pred2)^2)
}
which.min(val.errors2)

```


Lasso ***Still need to figure out****

```{r}
#library(glmnet)
x = heart_dat$output
y = model.matrix(output ~ ., data = heart_dat)[,-14]
grid = 10^seq(10,-2,length = 100)

lasso.mod = cv.glmnet(x,y,alpha = 1, lambda = grid, family = "binomial")


```

